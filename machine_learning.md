# Machine Learning
The list of questions are from: https://github.com/alexeygrigorev/data-science-interviews/blob/master/theory.md and the original source is this blog post: https://medium.com/data-science-insider/160-data-science-interview-questions-14dbd8bf0a08


> What is supervised machine learning?

> What is regression? What is linear regression?

> What are the different types of regression models?

> What are the main assumptions of linear regression?

> What’s the normal distribution? Why do we care about it?

> How do we check if a variable follows the normal distribution?

> What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices?

> What methods for solving linear regression do you know?

> What is gradient descent? How does it work?

> What is SGD  —  stochastic gradient descent? What’s the difference with the usual gradient descent?

### Evaluation
> Which metrics for evaluating regression models do you know?

> What is the bias-variance trade-off?

> What is overfitting?

> How to validate your models?

> Why do we need to split our data into three parts: train, validation, and test?

> Can you explain how cross-validation works?

> What is K-fold cross-validation?

> How do we choose K in K-fold cross-validation? What’s your favorite K?

### Classification
> What is classification? Which models would you use to solve a classification problem?

> What is logistic regression? When do we need to use it?

> Is logistic regression a linear model? Why?

> What is sigmoid? What does it do?

> How do we evaluate classification models?

> What do we do with categorical variables?

> Why do we need one-hot encoding?

> What is "curse of dimensionality"?

### Multi-collinearity
> What happens to our linear regression model if we have three columns in our data: x, y, z  —  and z is a sum of x and y?

> What happens to our linear regression model if the column z in the data is a sum of columns x and y and some random noise?

### Regularization
> What is regularization? Why do we need it?

> Which regularization techniques do you know?

> What kind of regularization techniques are applicable to linear models?

> How does L2 regularization look like in a linear model?

> How do we select the right regularization parameters?

> What’s the effect of L2 regularization on the weights of a linear model?

> How L1 regularization looks like in a linear model?

> What’s the difference between L2 and L1 regularization?

> Can we have both L1 and L2 regularization components in a linear model?

> When do we need to perform feature normalization for linear models? When it’s okay not to do it?

### Interpretation
> What’s the interpretation of the bias term in linear models?

> How do we interpret weights in linear models?

> If a weight for one variable is higher than for another  —  can we say that this variable is more important?

### Feature Selection
> What is feature selection? Why do we need it?

> Is feature selection important for linear models?

> Which feature selection techniques do you know?

> Can we use L1 regularization for feature selection?

> Can we use L2 regularization for feature selection?

### Clustering
> What is unsupervised learning?

> What is clustering? When do we need it?

> Do you know how K-means works?

> How to select K for K-means?

> What are the other clustering algorithms do you know?

### Dimensionality Reduction
> What is the curse of dimensionality? Why do we care about it?

> Do you know any dimensionality reduction techniques?

> What’s singular value decomposition? How is it typically used for machine learning?

### Time Series
> What is a time series?

> How is time series different from the usual regression problem?

> Which models do you know for solving time series problems?

> If there’s a trend in our series, how we can remove it? And why would we want to do it?

> You have a series with only one variable “y” measured at time t. How do predict “y” at time t+1? Which approaches would you use?

> You have a series with a variable “y” and a set of features. How do you predict “y” at t+1? Which approaches would you use?

> What are the problems with using trees for solving time series problems?
